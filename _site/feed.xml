

<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://localhost:4000/</id>
  <title>washing</title>
  <subtitle>my writing field</subtitle>
  <updated>2023-10-29T21:16:59+08:00</updated>
  <author>
    <name>washing</name>
    <uri>http://localhost:4000/</uri>
  </author>
  <link rel="self" type="application/atom+xml" href="http://localhost:4000/feed.xml"/>
  <link rel="alternate" type="text/html" hreflang="en"
    href="http://localhost:4000/"/>
  <generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator>
  <rights> © 2023 washing </rights>
  <icon>/assets/img/favicons/favicon.ico</icon>
  <logo>/assets/img/favicons/favicon-96x96.png</logo>


  
  <entry>
    <title>【DAILY READING】Language Is Not All You Need：Aligning Perception with Language Models</title>
    <link href="http://localhost:4000/posts/Language-Is-Not-All-You-Need-Aligning-Perception-with-Language-Models/" rel="alternate" type="text/html" title="【DAILY READING】Language Is Not All You Need：Aligning Perception with Language Models" />
    <published>2023-10-29T20:32:57+08:00</published>
  
    <updated>2023-10-29T20:47:44+08:00</updated>
  
    <id>http://localhost:4000/posts/Language-Is-Not-All-You-Need-Aligning-Perception-with-Language-Models/</id>
    <content src="http://localhost:4000/posts/Language-Is-Not-All-You-Need-Aligning-Perception-with-Language-Models/" />
    <author>
      <name>washing</name>
    </author>

  
    
    <category term="paper" />
    
  

  
    <summary>
      





      Conclusion By Myself
This paper introduced a Multimodal LLM, which combine image tasks and text tasks together.
It use CLIP ViT-L/14 model to representing the pic, and use a transformer based model MAGNETO to do the language model work.
The final result shows that going from LLMs to MLLMs enables new capabilities and opportunities.
There are three conclusion in this paper by authors:

  From LL...
    </summary>
  

  </entry>

  
  <entry>
    <title>【DAILY READING】In-Context Pretraining：Language Modeling Beyond Document Boundaries</title>
    <link href="http://localhost:4000/posts/In-Context-Pretraining-Language-Modeling-Beyond-Document-Boundaries/" rel="alternate" type="text/html" title="【DAILY READING】In-Context Pretraining：Language Modeling Beyond Document Boundaries" />
    <published>2023-10-29T20:15:59+08:00</published>
  
    <updated>2023-10-29T20:47:44+08:00</updated>
  
    <id>http://localhost:4000/posts/In-Context-Pretraining-Language-Modeling-Beyond-Document-Boundaries/</id>
    <content src="http://localhost:4000/posts/In-Context-Pretraining-Language-Modeling-Beyond-Document-Boundaries/" />
    <author>
      <name>washing</name>
    </author>

  
    
    <category term="paper" />
    
  

  
    <summary>
      





      Conclusion By Myself
Feed more related data to model at once can significantly enhance its performance across multiple aspects.
Use the cosine similarity to determine the similarity between any two documents: $s(d_i,d_j)=\cos(\rm E\it(d_i),\rm E\it(d_j))$.
Formulate the graph traversal problem as a maximum traveling salesman problem, and use greedy algorithms to solve it.
Abstract
Large languag...
    </summary>
  

  </entry>

  
  <entry>
    <title>【DAILY READING】CodeChain：Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules</title>
    <link href="http://localhost:4000/posts/CodeChain-Towards-Modular-Code-Generation-Through-Chain-of-Self-revisions-with-Representative-Sub-modules/" rel="alternate" type="text/html" title="【DAILY READING】CodeChain：Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules" />
    <published>2023-10-29T20:15:49+08:00</published>
  
    <updated>2023-10-29T20:47:44+08:00</updated>
  
    <id>http://localhost:4000/posts/CodeChain-Towards-Modular-Code-Generation-Through-Chain-of-Self-revisions-with-Representative-Sub-modules/</id>
    <content src="http://localhost:4000/posts/CodeChain-Towards-Modular-Code-Generation-Through-Chain-of-Self-revisions-with-Representative-Sub-modules/" />
    <author>
      <name>washing</name>
    </author>

  
    
    <category term="paper" />
    
  

  
    <summary>
      





      Conclusion By Myself
LLMs lack of the ability to write modularized code to solve complex task. This paper introduced a way of inference to solve this problem.
Concretely in two step:

  Use “chain-of-thought(CoT)” to prompt the LLM to get more modularized codes.
  Self-revisions by two inner step:
    
      Clustering these code.
      Use CoT prompt again and ask the llm to generate answer by...
    </summary>
  

  </entry>

  
  <entry>
    <title>【DAILY READING】Augmented Language Models：a Survey</title>
    <link href="http://localhost:4000/posts/Augmented-Language-Models-a-Survey/" rel="alternate" type="text/html" title="【DAILY READING】Augmented Language Models：a Survey" />
    <published>2023-10-29T20:15:39+08:00</published>
  
    <updated>2023-10-29T20:47:44+08:00</updated>
  
    <id>http://localhost:4000/posts/Augmented-Language-Models-a-Survey/</id>
    <content src="http://localhost:4000/posts/Augmented-Language-Models-a-Survey/" />
    <author>
      <name>washing</name>
    </author>

  
    
    <category term="paper" />
    
  

  
    <summary>
      





      Conclusion By Myself
Table of Contents
Abstract
This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools.
The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter.
LMs can leverage these augmentations separately or in comb...
    </summary>
  

  </entry>

  
  <entry>
    <title>【DAILY READING】Textbooks Are All You Need</title>
    <link href="http://localhost:4000/posts/Textbooks-Are-All-You-Need/" rel="alternate" type="text/html" title="【DAILY READING】Textbooks Are All You Need" />
    <published>2023-10-29T20:15:33+08:00</published>
  
    <updated>2023-10-29T20:47:44+08:00</updated>
  
    <id>http://localhost:4000/posts/Textbooks-Are-All-You-Need/</id>
    <content src="http://localhost:4000/posts/Textbooks-Are-All-You-Need/" />
    <author>
      <name>washing</name>
    </author>

  
    
    <category term="paper" />
    
  

  
    <summary>
      





      Summary of total paper
This paper gives an opinion is that the quality of the data has a higher cost performance on the size of model on specific targets now.
It proved this point of view by using Code as an example. A 1.3B model, pre-trained and finetuned on some textbooks-data, gives an extraordinary performance on HumanEval and MBPP.
Abstract
With the emerging of Transformer architecture, th...
    </summary>
  

  </entry>

</feed>


