<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="en" >
  <!-- The Head -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  >

  

  

  
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="【DAILY READING】CodeChain：Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules" />
<meta property="og:locale" content="en" />
<meta name="description" content="Conclusion By Myself LLMs lack of the ability to write modularized code to solve complex task. This paper introduced a way of inference to solve this problem. Concretely in two step: Use “chain-of-thought(CoT)” to prompt the LLM to get more modularized codes. Self-revisions by two inner step: Clustering these code. Use CoT prompt again and ask the llm to generate answer by its previous output. Abstract Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possible due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules. To address this gap, we propose CodeChain, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations. Concretely, CodeChain first instructs the LLM to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps: extracting and clustering the generated sub-modules and selecting the cluster representatives as the more generic and re-usable implementations augmenting the original chain-of-thought prompt with these selected module-implementations and instructing the LLM to re-generate new modularized solutions. We find that by naturally encouraging the LLM to reuse the previously developed and verified sub-modules, CodeChain can significantly boost both modularity as well as correctness of the generated solutions, achieving relative pass@1 improvements of 35% on APPS and 76% on CodeContents. It is shown to be effective on both OpenAI LLMs as well as open-sourced LLMs like WizardCoder. We also conduct comprehensive ablation studies with different methods of prompting, number of clusters, model sizes, program qualities, etc., to provide useful insights that underpin CodeChain’s success. Key Points By AI The long-term goal of artificial intelligence is to develop executable and functionally correct computer programs to solve complex problems. Large pre-trained language models (LLMs) have made unprecedented progress in recent years and have expanded to learning large-scale code data. Current state-of-the-art models cannot compare with skilled developers in highly complex programming tasks because their generation methods are too simple. Most methods using LLMs adopt a simple generation approach, where the model usually generates code solutions as a single overall code block. Another limitation is that the model generates a large number of solutions independently, hoping that one of them will pass all private test cases. Recent research has proposed a method of subsampling output, providing the model with problem descriptions and public test cases as input. Experienced developers will try to modify and reuse code developed previously until they are satisfied. Some recent works have addressed this issue, for example, using LLM for self-correction, improving generated solutions through feedback such as compiler error messages, test results, and natural language explanations. These methods only use independent feedback from each solution, ignoring the potential collective insights of all generated samples or their subcomponents. Inspired by the problem-solving process, we propose CodeChain, a novel reasoning framework to improve code generation in LLMs through a chain of self-correcting submodules. In CodeChain, we first introduce chain-of-thought prompts to indicate that the LLM should decompose solutions into modular paragraphs, each representing an abstract function for advanced logical subtasks. To exploit modularity in the program, we further improve the generation process through a series of self-corrections, each based on a set of sampled submodules. We first extract the submodules found in the generated program and group them into clusters, where we sample the central submodules and treat them as reusable code parts for self-correction. Then, we expand the original chain-of-thought prompts through these selected submodules and instruct the LLM to generate new modular solutions. In this way, the LLM can gain collective insights from the modular components of all past generated samples to improve its future generations, imitating the problem-solving process of experienced developers. Experimental results show that CodeChain can significantly improve the performance of LLMs and achieve state-of-the-art performance on challenging code tasks in APPS and CodeContests. CodeChain has improved the average pass@1 performance on APPS by over 35%, and on CodeContests by over 76%. We have also observed continuous improvements for both OpenAI LLM and open-source LLMs (such as WizardCoder). We have also conducted comprehensive ablation studies, including analyzing single-step and multi-step revisions, feedback types, and cluster numbers, to gain useful insights into success of CodeChain. Our work is closely related to the research of large-scale Transformer-based language models (LLMs). Originally designed for natural language processing, these models have been expanded to learn large-scale code data and are skilled at understanding the context of programming languages and generating outputs. More directly related is recent work aimed at improving code generation quality through output feedback, which introduced a simple filtering method by only selecting output samples that passed public test cases. Some related work aims to improve generation quality through iterative self-revision, which uses the test results of public test cases as feedback for the model to revise its code. File [2310.08992] CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules (arxiv.org)" />
<meta property="og:description" content="Conclusion By Myself LLMs lack of the ability to write modularized code to solve complex task. This paper introduced a way of inference to solve this problem. Concretely in two step: Use “chain-of-thought(CoT)” to prompt the LLM to get more modularized codes. Self-revisions by two inner step: Clustering these code. Use CoT prompt again and ask the llm to generate answer by its previous output. Abstract Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possible due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules. To address this gap, we propose CodeChain, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations. Concretely, CodeChain first instructs the LLM to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps: extracting and clustering the generated sub-modules and selecting the cluster representatives as the more generic and re-usable implementations augmenting the original chain-of-thought prompt with these selected module-implementations and instructing the LLM to re-generate new modularized solutions. We find that by naturally encouraging the LLM to reuse the previously developed and verified sub-modules, CodeChain can significantly boost both modularity as well as correctness of the generated solutions, achieving relative pass@1 improvements of 35% on APPS and 76% on CodeContents. It is shown to be effective on both OpenAI LLMs as well as open-sourced LLMs like WizardCoder. We also conduct comprehensive ablation studies with different methods of prompting, number of clusters, model sizes, program qualities, etc., to provide useful insights that underpin CodeChain’s success. Key Points By AI The long-term goal of artificial intelligence is to develop executable and functionally correct computer programs to solve complex problems. Large pre-trained language models (LLMs) have made unprecedented progress in recent years and have expanded to learning large-scale code data. Current state-of-the-art models cannot compare with skilled developers in highly complex programming tasks because their generation methods are too simple. Most methods using LLMs adopt a simple generation approach, where the model usually generates code solutions as a single overall code block. Another limitation is that the model generates a large number of solutions independently, hoping that one of them will pass all private test cases. Recent research has proposed a method of subsampling output, providing the model with problem descriptions and public test cases as input. Experienced developers will try to modify and reuse code developed previously until they are satisfied. Some recent works have addressed this issue, for example, using LLM for self-correction, improving generated solutions through feedback such as compiler error messages, test results, and natural language explanations. These methods only use independent feedback from each solution, ignoring the potential collective insights of all generated samples or their subcomponents. Inspired by the problem-solving process, we propose CodeChain, a novel reasoning framework to improve code generation in LLMs through a chain of self-correcting submodules. In CodeChain, we first introduce chain-of-thought prompts to indicate that the LLM should decompose solutions into modular paragraphs, each representing an abstract function for advanced logical subtasks. To exploit modularity in the program, we further improve the generation process through a series of self-corrections, each based on a set of sampled submodules. We first extract the submodules found in the generated program and group them into clusters, where we sample the central submodules and treat them as reusable code parts for self-correction. Then, we expand the original chain-of-thought prompts through these selected submodules and instruct the LLM to generate new modular solutions. In this way, the LLM can gain collective insights from the modular components of all past generated samples to improve its future generations, imitating the problem-solving process of experienced developers. Experimental results show that CodeChain can significantly improve the performance of LLMs and achieve state-of-the-art performance on challenging code tasks in APPS and CodeContests. CodeChain has improved the average pass@1 performance on APPS by over 35%, and on CodeContests by over 76%. We have also observed continuous improvements for both OpenAI LLM and open-source LLMs (such as WizardCoder). We have also conducted comprehensive ablation studies, including analyzing single-step and multi-step revisions, feedback types, and cluster numbers, to gain useful insights into success of CodeChain. Our work is closely related to the research of large-scale Transformer-based language models (LLMs). Originally designed for natural language processing, these models have been expanded to learn large-scale code data and are skilled at understanding the context of programming languages and generating outputs. More directly related is recent work aimed at improving code generation quality through output feedback, which introduced a simple filtering method by only selecting output samples that passed public test cases. Some related work aims to improve generation quality through iterative self-revision, which uses the test results of public test cases as feedback for the model to revise its code. File [2310.08992] CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules (arxiv.org)" />
<link rel="canonical" href="http://localhost:4000/posts/CodeChain-Towards-Modular-Code-Generation-Through-Chain-of-Self-revisions-with-Representative-Sub-modules/" />
<meta property="og:url" content="http://localhost:4000/posts/CodeChain-Towards-Modular-Code-Generation-Through-Chain-of-Self-revisions-with-Representative-Sub-modules/" />
<meta property="og:site_name" content="washing" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-10-29T20:15:49+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="【DAILY READING】CodeChain：Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules" />
<meta name="twitter:site" content="@washing77231482" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-10-29T20:47:44+08:00","datePublished":"2023-10-29T20:15:49+08:00","description":"Conclusion By Myself LLMs lack of the ability to write modularized code to solve complex task. This paper introduced a way of inference to solve this problem. Concretely in two step: Use “chain-of-thought(CoT)” to prompt the LLM to get more modularized codes. Self-revisions by two inner step: Clustering these code. Use CoT prompt again and ask the llm to generate answer by its previous output. Abstract Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possible due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules. To address this gap, we propose CodeChain, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations. Concretely, CodeChain first instructs the LLM to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps: extracting and clustering the generated sub-modules and selecting the cluster representatives as the more generic and re-usable implementations augmenting the original chain-of-thought prompt with these selected module-implementations and instructing the LLM to re-generate new modularized solutions. We find that by naturally encouraging the LLM to reuse the previously developed and verified sub-modules, CodeChain can significantly boost both modularity as well as correctness of the generated solutions, achieving relative pass@1 improvements of 35% on APPS and 76% on CodeContents. It is shown to be effective on both OpenAI LLMs as well as open-sourced LLMs like WizardCoder. We also conduct comprehensive ablation studies with different methods of prompting, number of clusters, model sizes, program qualities, etc., to provide useful insights that underpin CodeChain’s success. Key Points By AI The long-term goal of artificial intelligence is to develop executable and functionally correct computer programs to solve complex problems. Large pre-trained language models (LLMs) have made unprecedented progress in recent years and have expanded to learning large-scale code data. Current state-of-the-art models cannot compare with skilled developers in highly complex programming tasks because their generation methods are too simple. Most methods using LLMs adopt a simple generation approach, where the model usually generates code solutions as a single overall code block. Another limitation is that the model generates a large number of solutions independently, hoping that one of them will pass all private test cases. Recent research has proposed a method of subsampling output, providing the model with problem descriptions and public test cases as input. Experienced developers will try to modify and reuse code developed previously until they are satisfied. Some recent works have addressed this issue, for example, using LLM for self-correction, improving generated solutions through feedback such as compiler error messages, test results, and natural language explanations. These methods only use independent feedback from each solution, ignoring the potential collective insights of all generated samples or their subcomponents. Inspired by the problem-solving process, we propose CodeChain, a novel reasoning framework to improve code generation in LLMs through a chain of self-correcting submodules. In CodeChain, we first introduce chain-of-thought prompts to indicate that the LLM should decompose solutions into modular paragraphs, each representing an abstract function for advanced logical subtasks. To exploit modularity in the program, we further improve the generation process through a series of self-corrections, each based on a set of sampled submodules. We first extract the submodules found in the generated program and group them into clusters, where we sample the central submodules and treat them as reusable code parts for self-correction. Then, we expand the original chain-of-thought prompts through these selected submodules and instruct the LLM to generate new modular solutions. In this way, the LLM can gain collective insights from the modular components of all past generated samples to improve its future generations, imitating the problem-solving process of experienced developers. Experimental results show that CodeChain can significantly improve the performance of LLMs and achieve state-of-the-art performance on challenging code tasks in APPS and CodeContests. CodeChain has improved the average pass@1 performance on APPS by over 35%, and on CodeContests by over 76%. We have also observed continuous improvements for both OpenAI LLM and open-source LLMs (such as WizardCoder). We have also conducted comprehensive ablation studies, including analyzing single-step and multi-step revisions, feedback types, and cluster numbers, to gain useful insights into success of CodeChain. Our work is closely related to the research of large-scale Transformer-based language models (LLMs). Originally designed for natural language processing, these models have been expanded to learn large-scale code data and are skilled at understanding the context of programming languages and generating outputs. More directly related is recent work aimed at improving code generation quality through output feedback, which introduced a simple filtering method by only selecting output samples that passed public test cases. Some related work aims to improve generation quality through iterative self-revision, which uses the test results of public test cases as feedback for the model to revise its code. File [2310.08992] CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules (arxiv.org)","headline":"【DAILY READING】CodeChain：Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/CodeChain-Towards-Modular-Code-Generation-Through-Chain-of-Self-revisions-with-Representative-Sub-modules/"},"url":"http://localhost:4000/posts/CodeChain-Towards-Modular-Code-Generation-Through-Chain-of-Self-revisions-with-Representative-Sub-modules/"}</script>
<!-- End Jekyll SEO tag -->

  

  <title>【DAILY READING】CodeChain：Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules | washing
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">
<link rel="manifest" href="/assets/img/favicons/site.webmanifest">
<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="washing">
<meta name="application-name" content="washing">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin>
    
      <link rel="preconnect" href="https://fonts.googleapis.com" >
      <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
    
      <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
    

    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">
  

  <!-- GA -->
  

  <!-- Bootstrap -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.2/css/all.min.css">

  <link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css">

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.21.2/dist/tocbot.min.css">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css">
  

  
    <!-- Manific Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css">
  

  <!-- JavaScript -->

  
    <!-- Switch the mode between dark and light. -->

<script type="text/javascript">
  class ModeToggle {
    static get MODE_KEY() {
      return 'mode';
    }
    static get MODE_ATTR() {
      return 'data-mode';
    }
    static get DARK_MODE() {
      return 'dark';
    }
    static get LIGHT_MODE() {
      return 'light';
    }
    static get ID() {
      return 'mode-toggle';
    }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      let self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addEventListener('change', () => {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }
          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.notify();
      });
    } /* constructor() */

    get sysDarkPrefers() {
      return window.matchMedia('(prefers-color-scheme: dark)');
    }

    get isSysDarkPrefer() {
      return this.sysDarkPrefers.matches;
    }

    get isDarkMode() {
      return this.mode === ModeToggle.DARK_MODE;
    }

    get isLightMode() {
      return this.mode === ModeToggle.LIGHT_MODE;
    }

    get hasMode() {
      return this.mode != null;
    }

    get mode() {
      return sessionStorage.getItem(ModeToggle.MODE_KEY);
    }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    setDark() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      document.documentElement.removeAttribute(ModeToggle.MODE_ATTR);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    /* Notify another plugins that the theme mode has changed */
    notify() {
      window.postMessage(
        {
          direction: ModeToggle.ID,
          message: this.modeStatus
        },
        '*'
      );
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }
        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }
      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.notify();
    } /* flipMode() */
  } /* ModeToggle */

  const modeToggle = new ModeToggle();
</script>

  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end">
  <header class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle">
      
        
        <img src="/assets/img/avatar/dragonball.jpg" width="112" height="112" alt="avatar" onerror="this.style.display='none'">
      
    </a>

    <h1 class="site-title">
      <a href="/">washing</a>
    </h1>
    <p class="site-subtitle fst-italic mb-0">The journey is long and arduous, my heart is calm and resolute.</p>
  </header>
  <!-- .profile-wrapper -->

  <nav class="flex-column flex-grow-1 w-100 ps-0">
    <ul class="nav">
      <!-- home -->
      <li class="nav-item">
        <a href="/" class="nav-link">
          <i class="fa-fw fas fa-home"></i>
          <span>HOME</span>
        </a>
      </li>
      <!-- the real tabs -->
      
        <li class="nav-item">
          <a href="/categories/" class="nav-link">
            <i class="fa-fw fas fa-stream"></i>
            

            <span>CATEGORIES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/tags/" class="nav-link">
            <i class="fa-fw fas fa-tags"></i>
            

            <span>TAGS</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/archives/" class="nav-link">
            <i class="fa-fw fas fa-archive"></i>
            

            <span>ARCHIVES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/about/" class="nav-link">
            <i class="fa-fw fas fa-info-circle"></i>
            

            <span>ABOUT</span>
          </a>
        </li>
        <!-- .nav-item -->
      
    </ul>
  </nav>

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button type="button" class="mode-toggle btn" aria-label="Switch Mode">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/washing1127"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/washing77231482"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fa-brands fa-x-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['washing1127','gmail.com'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</aside>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div class="container d-flex flex-column px-xxl-5">
        <!-- The Top Bar -->

<header id="topbar-wrapper" aria-label="Top Bar">
  <div
    id="topbar"
    class="d-flex align-items-center justify-content-between px-lg-3 h-100"
  >
    <nav id="breadcrumb" aria-label="Breadcrumb">
      

      
        
          
            <span>
              <a href="/">
                Home
              </a>
            </span>

          
        
          
        
          
            
              <span>【DAILY READING】CodeChain：Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules</span>
            

          
        
      
    </nav>
    <!-- endof #breadcrumb -->

    <button type="button" id="sidebar-trigger" class="btn btn-link">
      <i class="fas fa-bars fa-fw"></i>
    </button>

    <div id="topbar-title">
      Post
    </div>

    <button type="button" id="search-trigger" class="btn btn-link">
      <i class="fas fa-search fa-fw"></i>
    </button>

    <search class="align-items-center ms-3 ms-lg-0">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="Search..."
      >
    </search>
    <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
  </div>
</header>


        <div class="row flex-grow-1">
          <main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->



<!-- Handle images -->





<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  

  
  

  

  
  

  

  
  

  




<!-- return -->




<article class="px-1">
  <header>
    <h1 data-toc-skip>【DAILY READING】CodeChain：Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules</h1>

    <div class="post-meta text-muted">
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1698581749"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Oct 29, 2023
</time>

      </span>

      <!-- lastmod date -->
      
        <span>
          Updated
          <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1698583664"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Oct 29, 2023
</time>

        </span>
      

      

      <div class="d-flex justify-content-between">
        <!-- author(s) -->
        <span>
          

          By

          <em>
            
              <a href="https://www.cnblogs.com/washing/">washing</a>
            
          </em>
        </span>

        <!-- read time -->
        <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="878 words"
>
  <em>4 min</em> read</span>

      </div>
      <!-- .d-flex -->
    </div>
    <!-- .post-meta -->
  </header>

  <div class="content">
    <h1 id="conclusion-by-myself">Conclusion By Myself</h1>
<p>LLMs lack of the ability to write modularized code to solve complex task. This paper introduced a way of inference to solve this problem.
Concretely in two step:</p>
<ol>
  <li>Use “chain-of-thought(CoT)” to prompt the LLM to get more modularized codes.</li>
  <li>Self-revisions by two inner step:
    <ol>
      <li>Clustering these code.</li>
      <li>Use CoT prompt again and ask the llm to generate answer by its previous output.
        <h1 id="abstract">Abstract</h1>
        <p>Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks.
However, solving more complex and competitive programming tasks is still quite challenging for these models - possible due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules.
To address this gap, we propose CodeChain, a novel framework for <strong>inference</strong> that elicits modularized code generation through a <strong>chain of self-revisions</strong>, each being guided by some representative sub-modules generated in previous iterations.
Concretely, CodeChain first instructs the LLM to generate modularized codes through <strong>chain-of-thought prompting</strong>. Then it applies a chain of self-revisions by iterating the two steps:</p>
        <ul>
          <li><strong>extracting and clustering</strong> the generated sub-modules and selecting the cluster representatives as the more generic and re-usable implementations</li>
          <li><strong>augmenting the original chain-of-thought prompt</strong> with these selected module-implementations and instructing the LLM to re-generate new modularized solutions.
We find that by naturally encouraging the LLM to reuse the previously developed and verified sub-modules, CodeChain can significantly boost both modularity as well as correctness of the generated solutions, achieving relative pass@1 improvements of 35% on APPS and 76% on CodeContents.
It is shown to be effective on both OpenAI LLMs as well as open-sourced LLMs like WizardCoder. We also conduct comprehensive ablation studies with different methods of prompting, number of clusters, model sizes, program qualities, etc., to provide useful insights that underpin CodeChain’s success.
            <h1 id="key-points-by-ai">Key Points By AI</h1>
          </li>
        </ul>
      </li>
    </ol>
  </li>
  <li>The long-term goal of artificial intelligence is to develop executable and functionally correct computer programs to solve complex problems.</li>
  <li>Large pre-trained language models (LLMs) have made unprecedented progress in recent years and have expanded to learning large-scale code data.</li>
  <li>Current state-of-the-art models cannot compare with skilled developers in highly complex programming tasks because their generation methods are too simple.</li>
  <li>Most methods using LLMs adopt a simple generation approach, where the model usually generates code solutions as a single overall code block.</li>
  <li>Another limitation is that the model generates a large number of solutions independently, hoping that one of them will pass all private test cases.</li>
  <li>Recent research has proposed a method of subsampling output, providing the model with problem descriptions and public test cases as input.</li>
  <li>Experienced developers will try to modify and reuse code developed previously until they are satisfied.</li>
  <li>Some recent works have addressed this issue, for example, using LLM for self-correction, improving generated solutions through feedback such as compiler error messages, test results, and natural language explanations.</li>
  <li>These methods only use independent feedback from each solution, ignoring the potential collective insights of all generated samples or their subcomponents.</li>
  <li>Inspired by the problem-solving process, we propose CodeChain, a novel reasoning framework to improve code generation in LLMs through a chain of self-correcting submodules.</li>
  <li>In CodeChain, we first introduce chain-of-thought prompts to indicate that the LLM should decompose solutions into modular paragraphs, each representing an abstract function for advanced logical subtasks.</li>
  <li>To exploit modularity in the program, we further improve the generation process through a series of self-corrections, each based on a set of sampled submodules.</li>
  <li>We first extract the submodules found in the generated program and group them into clusters, where we sample the central submodules and treat them as reusable code parts for self-correction.</li>
  <li>Then, we expand the original chain-of-thought prompts through these selected submodules and instruct the LLM to generate new modular solutions.</li>
  <li>In this way, the LLM can gain collective insights from the modular components of all past generated samples to improve its future generations, imitating the problem-solving process of experienced developers.</li>
  <li>Experimental results show that CodeChain can significantly improve the performance of LLMs and achieve state-of-the-art performance on challenging code tasks in APPS and CodeContests.</li>
  <li>CodeChain has improved the average pass@1 performance on APPS by over 35%, and on CodeContests by over 76%. We have also observed continuous improvements for both OpenAI LLM and open-source LLMs (such as WizardCoder).</li>
  <li>We have also conducted comprehensive ablation studies, including analyzing single-step and multi-step revisions, feedback types, and cluster numbers, to gain useful insights into success of CodeChain.</li>
  <li>Our work is closely related to the research of large-scale Transformer-based language models (LLMs).</li>
  <li>Originally designed for natural language processing, these models have been expanded to learn large-scale code data and are skilled at understanding the context of programming languages and generating outputs.</li>
  <li>More directly related is recent work aimed at improving code generation quality through output feedback, which introduced a simple filtering method by only selecting output samples that passed public test cases.</li>
  <li>Some related work aims to improve generation quality through iterative self-revision, which uses the test results of public test cases as feedback for the model to revise its code.
    <h1 id="file">File</h1>
    <p><a href="https://arxiv.org/abs/2310.08992">[2310.08992] CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules (arxiv.org)</a></p>
  </li>
</ol>

  </div>

  <div class="post-tail-wrapper text-muted">
    <!-- categories -->
    
      <div class="post-meta mb-3">
        <i class="far fa-folder-open fa-fw me-1"></i>
        
          <a href="/categories/paper/">paper</a>
      </div>
    

    <!-- tags -->
    
      <div class="post-tags">
        <i class="fa fa-tags fa-fw me-1"></i>
        
          <a
            href="/tags/daily-reading/"
            class="post-tag no-text-decoration"
          >daily reading</a>
        
      </div>
    

    <div
      class="
        post-tail-bottom
        d-flex justify-content-between align-items-center mt-5 pb-2
      "
    >
      <div class="license-wrapper">
        
          

          This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.
        
      </div>

      <!-- Post sharing snippet -->

<div class="share-wrapper d-flex align-items-center">
  <span class="share-label text-muted me-1">Share</span>
  <span class="share-icons">
    
    
    

    
      
      <a
        href="https://twitter.com/intent/tweet?text=%E3%80%90DAILY%20READING%E3%80%91CodeChain%EF%BC%9ATowards%20Modular%20Code%20Generation%20Through%20Chain%20of%20Self-revisions%20with%20Representative%20Sub-modules%20-%20washing&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FCodeChain-Towards-Modular-Code-Generation-Through-Chain-of-Self-revisions-with-Representative-Sub-modules%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Twitter"
        target="_blank"
        rel="noopener"
        aria-label="Twitter"
      >
        <i class="fa-fw fa-brands fa-square-x-twitter"></i>
      </a>
    
      
      <a
        href="https://www.facebook.com/sharer/sharer.php?title=%E3%80%90DAILY%20READING%E3%80%91CodeChain%EF%BC%9ATowards%20Modular%20Code%20Generation%20Through%20Chain%20of%20Self-revisions%20with%20Representative%20Sub-modules%20-%20washing&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FCodeChain-Towards-Modular-Code-Generation-Through-Chain-of-Self-revisions-with-Representative-Sub-modules%2F"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Facebook"
        target="_blank"
        rel="noopener"
        aria-label="Facebook"
      >
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    
      
      <a
        href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2FCodeChain-Towards-Modular-Code-Generation-Through-Chain-of-Self-revisions-with-Representative-Sub-modules%2F&text=%E3%80%90DAILY%20READING%E3%80%91CodeChain%EF%BC%9ATowards%20Modular%20Code%20Generation%20Through%20Chain%20of%20Self-revisions%20with%20Representative%20Sub-modules%20-%20washing"
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Telegram"
        target="_blank"
        rel="noopener"
        aria-label="Telegram"
      >
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <button
      id="copy-link"
      aria-label="Copy link"
      class="btn small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="Copy link"
      data-title-succeed="Link copied successfully!"
    >
      <i class="fa-fw fas fa-link pe-none"></i>
    </button>
  </span>
</div>

    </div>
    <!-- .post-tail-bottom -->
  </div>
  <!-- div.post-tail-wrapper -->
</article>


            
          </main>

          <!-- panel -->
          <aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 mb-5 text-muted">
            <div class="access">
              <!-- Get the last 5 posts from lastmod list. -->














  <section id="access-lastmod">
    <h2 class="panel-heading">Recently Updated</h2>
    <ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Distilling-the-Knowledge-in-a-Neural-Network/">【DAILY READING】Distilling the Knowledge in a Neural Network</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/DistilBERT,-a-distilled-version-of-BERT-smaller,-faster,-cheaper-and-lighter/">【DAILY READING】DistilBERT, a distilled version of BERT：smaller, faster, cheaper and lighter</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/iTransformer-Inverted-Transformer-Are-Effective-for-Time-Series-Forecasting/">【DAILY READING】iTransformer：Inverted Transformer Are Effective for Time Series Forecasting</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Deep-contextualized-word-realizations/">【DAILY READING】Deep contextualized word realizations</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/Efficient-Estimation-of-Word-Representations-in-Vector-Space/">【DAILY READING】Efficient Estimation of Word Representations in Vector Space</a>
        </li>
      
    </ul>
  </section>
  <!-- #access-lastmod -->


              <!-- The trending tags list -->















  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/daily-reading/">daily reading</a>
      
    </div>
  </section>


            </div>

            
              
              




            
          </aside>
        </div>

        <div class="row">
          <!-- tail -->
          <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              
              <!-- Recommend the other 3 posts according to the tags and categories of the current post. -->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->














  

  

  

  

  

  

  

  

  

  

  

  

  

  
    
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  











  <aside id="related-posts" aria-labelledby="related-label">
    <h3 class="mb-4" id="related-label">Further Reading</h3>
    <nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        <article class="col">
          <a href="/posts/Distilling-the-Knowledge-in-a-Neural-Network/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1698581701"
  data-df="ll"
  
>
  Oct 29, 2023
</time>

              <h4 class="pt-0 my-2">【DAILY READING】Distilling the Knowledge in a Neural Network</h4>
              <div class="text-muted">
                <p>
                  





                  Conclusion By Myself
This paper introduced that it is helpful to train a little model with a big model. 
The specific method is train a big model with a lot of data and make it performance good. Th...
                </p>
              </div>
            </div>
          </a>
        </article>
      
        <article class="col">
          <a href="/posts/DistilBERT,-a-distilled-version-of-BERT-smaller,-faster,-cheaper-and-lighter/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1698581711"
  data-df="ll"
  
>
  Oct 29, 2023
</time>

              <h4 class="pt-0 my-2">【DAILY READING】DistilBERT, a distilled version of BERT：smaller, faster, cheaper and lighter</h4>
              <div class="text-muted">
                <p>
                  





                  Conclusion By Myself
This paper is about distilling BERT model. It is not so hard to read, but because the lacking of the distilling knowledge, I still can not clearly know what it said, Aha.
It in...
                </p>
              </div>
            </div>
          </a>
        </article>
      
        <article class="col">
          <a href="/posts/iTransformer-Inverted-Transformer-Are-Effective-for-Time-Series-Forecasting/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1698581717"
  data-df="ll"
  
>
  Oct 29, 2023
</time>

              <h4 class="pt-0 my-2">【DAILY READING】iTransformer：Inverted Transformer Are Effective for Time Series Forecasting</h4>
              <div class="text-muted">
                <p>
                  





                  Conclusion By Myself
Honestly, this paper is not so clear to me. So I came back to this article Transformer王者归来！ where introduced the paper to me.
This is a specific aspect of transformer on time s...
                </p>
              </div>
            </div>
          </a>
        </article>
      
    </nav>
  </aside>
  <!-- #related-posts -->


            
              
              <!-- Navigation buttons at the bottom of the post. -->

<nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation">
  
  

  
    <a
      href="/posts/Augmented-Language-Models-a-Survey/"
      class="btn btn-outline-primary"
      aria-label="Older"
    >
      <p>【DAILY READING】Augmented Language Models：a Survey</p>
    </a>
  

  
    <a
      href="/posts/In-Context-Pretraining-Language-Modeling-Beyond-Document-Boundaries/"
      class="btn btn-outline-primary"
      aria-label="Newer"
    >
      <p>【DAILY READING】In-Context Pretraining：Language Modeling Beyond Document Boundaries</p>
    </a>
  
</nav>

            
              
              <!--  The comments switcher -->


            

            <!-- The Footer -->

<footer
  aria-label="Site Info"
  class="
    d-flex flex-column justify-content-center text-muted
    flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3
  "
>
  <p>
    ©
    <time>2023</time>
    <a href="https://www.cnblogs.com/washing/">washing</a>.
    
      <span
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."
      >Some rights reserved.</span>
    
  </p>

  <p>Using the <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>
  </p>
</footer>

          </div>
        </div>

        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-11 content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/daily-reading/">daily reading</a>
      
    </div>
  </section>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>

      <aside aria-label="Scroll to Top">
        <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow">
          <i class="fas fa-angle-up"></i>
        </button>
      </aside>
    </div>

    <div id="mask"></div>

    
      <aside
  id="notification"
  class="toast"
  role="alert"
  aria-live="assertive"
  aria-atomic="true"
  data-bs-animation="true"
  data-bs-autohide="false"
>
  <div class="toast-header">
    <button
      type="button"
      class="btn-close ms-auto"
      data-bs-dismiss="toast"
      aria-label="Close"
    ></button>
  </div>
  <div class="toast-body text-center pt-0">
    <p class="px-2 mb-3">A new version of content is available.</p>
    <button type="button" class="btn btn-primary" aria-label="Update">
      Update
    </button>
  </div>
</aside>

    

    <!-- JavaScripts -->

    <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script src="https://cdn.jsdelivr.net/combine/npm/jquery@3.7.1/dist/jquery.min.js,npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js,npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/magnific-popup@1.1.0/dist/jquery.magnific-popup.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.10/dayjs.min.js,npm/dayjs@1.11.10/locale/en.min.js,npm/dayjs@1.11.10/plugin/relativeTime.min.js,npm/dayjs@1.11.10/plugin/localizedFormat.min.js"></script>






<script defer src="/assets/js/dist/post.min.js"></script>


  <!-- MathJax -->
  <script>
    /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */
    MathJax = {
      tex: {
        /* start/end delimiter pairs for in-line math */
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ],
        /* start/end delimiter pairs for display math */
        displayMath: [
          ['$$', '$$'],
          ['\\[', '\\]']
        ]
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>





    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  /* Note: dependent library will be loaded in `js-selector.html` */
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('search-results'),
    json: '/assets/js/data/search.json',
    searchResultTemplate: '  <article class="px-1 px-sm-2 px-lg-4 px-xl-0">    <header>      <h2><a href="{url}">{title}</a></h2>      <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">        {categories}        {tags}      </div>    </header>    <p>{snippet}</p>  </article>',
    noResultsText: '<p class="mt-5"></p>',
    templateMiddleware: function(prop, value, template) {
      if (prop === 'categories') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
        }
      }

      if (prop === 'tags') {
        if (value === '') {
          return `${value}`;
        } else {
          return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
        }
      }
    }
  });
</script>

  </body>
</html>

