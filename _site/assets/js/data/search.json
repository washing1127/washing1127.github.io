[
  
  {
    "title": "【DAILY READING】Language Is Not All You Need：Aligning Perception with Language Models",
    "url": "/posts/Language-Is-Not-All-You-Need-Aligning-Perception-with-Language-Models/",
    "categories": "paper",
    "tags": "daily reading",
    "date": "2023-10-29 20:32:57 +0800",
    





    
    "snippet": "Conclusion By MyselfThis paper introduced a Multimodal LLM, which combine image tasks and text tasks together.It use CLIP ViT-L/14 model to representing the pic, and use a transformer based model M...",
    "content": "Conclusion By MyselfThis paper introduced a Multimodal LLM, which combine image tasks and text tasks together.It use CLIP ViT-L/14 model to representing the pic, and use a transformer based model MAGNETO to do the language model work.The final result shows that going from LLMs to MLLMs enables new capabilities and opportunities.There are three conclusion in this paper by authors:  From LLMs to MLLMs:          enable LLMs to get knowledge beyond text description.      opens a new task door to LLMs.      unified various APIs.        Language models as general-purpose interface  New capabilities of MLLMs:          zero- and few-shot      nonverbal reasoning      multi-turn interactions      AbstractA big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence.In this work, we introduce KOSMOS-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot).Specifically, we train KOSMOS-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data.We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates of finetuning.Experimental results show that KOSMOS-1 achieves impressive performance on  language understanding, generation, and even OCR-free NLP (directly fed with document images)  perception-language tasks, including multimodal dialogue, image captioning, visual question answering  vision tasks, such as image recognition with descriptions (specifying classification via text instructions).We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language.In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.Key Points By AI  From LLMs to Multimodal Large Language Model (MLLM)  Large Language Models (LLMs) have successfully served as general purpose interface across various natural language tasks.  There is still large performance gap between the current model and the average level of adults, KOSMOS-1 demonstrates the potential of MLLMs to perform zero-shot nonverbal reasoning by aligning perception with language models.  Multimodal Chain-of-Thought Prompting allows large language models to generate a series of reasoning steps and decompose a multi-step problem into intermediate steps.  We observe that providing descriptions in context can significantly improve the accuracy of image classification.File[2302.14045] Language Is Not All You Need: Aligning Perception with Language Models (arxiv.org)"
  },
  
  {
    "title": "【DAILY READING】In-Context Pretraining：Language Modeling Beyond Document Boundaries",
    "url": "/posts/In-Context-Pretraining-Language-Modeling-Beyond-Document-Boundaries/",
    "categories": "paper",
    "tags": "daily reading",
    "date": "2023-10-29 20:15:59 +0800",
    





    
    "snippet": "Conclusion By MyselfFeed more related data to model at once can significantly enhance its performance across multiple aspects.Use the cosine similarity to determine the similarity between any two d...",
    "content": "Conclusion By MyselfFeed more related data to model at once can significantly enhance its performance across multiple aspects.Use the cosine similarity to determine the similarity between any two documents: $s(d_i,d_j)=\\cos(\\rm E\\it(d_i),\\rm E\\it(d_j))$.Formulate the graph traversal problem as a maximum traveling salesman problem, and use greedy algorithms to solve it.AbstractLarge language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion.Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries.We can do In-Context Pretraining by simply changing the document ordering so that each context contains related document sorting problem is challenging. There are billions of documents and we would like to sort to maximize contextual similarity for every document without repeating any data.To do this, we introduce approximate algorithms for finding related documents with efficient nearest neighbor search and constructing coherent input contexts with a graph traversal algorithm.Our experiments show In-Context Pretraining offers a simple and scalable approach to significantly enhance LMs’ performance: we see notable improvements in tasks that require more complex contextual reasoning, including in-context learning (+8%), reading comprehension (+15%), faithfulness to previous contexts (+16%), long-context reasoning (+5%), and retrieval augmentation (+9%).Key Points By AI  Large-scale language models predict tokens based on context and are widely applied to various tasks.  Existing LMs have difficulty understanding complex context, such as accurately following instructions or performing poorly in context learning.  The IN-CONTEXT PRETRAINING method is proposed, which creates a coherent input context by combining related documents, allowing LMs to access long relevant contexts and provide pre-training signals beyond document boundaries.  IN-CONTEXT PRETRAINING only changes the document order and can be intergraded into the existing pre-training process.  Existing pre-training methods connect random documents together, but this method cannot provide more learning signals for LMs.  IN-CONTEXT PRETRAINING generates a more coherent input context by connecting semantically related documents.  IN-CONTEXT PRETRAINING consists of two steps:          Finding relevant documents in the pre-training corpus.      Using these relevant documents to construct the input context.        To find relevant documents, a retrieval model links documents in the pre-training corpus, and retrieval scores are used to eliminate approximate duplicate documents.  The IN-CONTENT PRETRAINING method demonstrates strong language modeling and downstream task performance on all model scales, such as context learning, reading comprehension, and long context reasoning.  The improvements brought about by IN-CONTEXT PRETRAINING include: an average increase of 8% in context learning across 8 datasets; an average improvement of 15% in reading comprehension tasks; output results that are more faithful to the previous context (+16%); a 5% improvement in long context reasoning; and a 9% gain in retrieval enhancement when using external knowledge from sources like Wikipedia.  The results show that by simply changing the order of the pre-training documents, IN-CONTEXT PRETRAINING provides a scalable and simple method to significantly enhance understanding and reasoning about the entire context.    File    [2310.10638] In-Context Pretraining: Language Modeling Beyond Document Boundaries (arxiv.org)  "
  },
  
  {
    "title": "【DAILY READING】CodeChain：Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules",
    "url": "/posts/CodeChain-Towards-Modular-Code-Generation-Through-Chain-of-Self-revisions-with-Representative-Sub-modules/",
    "categories": "paper",
    "tags": "daily reading",
    "date": "2023-10-29 20:15:49 +0800",
    





    
    "snippet": "Conclusion By MyselfLLMs lack of the ability to write modularized code to solve complex task. This paper introduced a way of inference to solve this problem.Concretely in two step:  Use “chain-of-t...",
    "content": "Conclusion By MyselfLLMs lack of the ability to write modularized code to solve complex task. This paper introduced a way of inference to solve this problem.Concretely in two step:  Use “chain-of-thought(CoT)” to prompt the LLM to get more modularized codes.  Self-revisions by two inner step:          Clustering these code.      Use CoT prompt again and ask the llm to generate answer by its previous output.        Abstract        Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks.However, solving more complex and competitive programming tasks is still quite challenging for these models - possible due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules.To address this gap, we propose CodeChain, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations.Concretely, CodeChain first instructs the LLM to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps:                  extracting and clustering the generated sub-modules and selecting the cluster representatives as the more generic and re-usable implementations          augmenting the original chain-of-thought prompt with these selected module-implementations and instructing the LLM to re-generate new modularized solutions.We find that by naturally encouraging the LLM to reuse the previously developed and verified sub-modules, CodeChain can significantly boost both modularity as well as correctness of the generated solutions, achieving relative pass@1 improvements of 35% on APPS and 76% on CodeContents.It is shown to be effective on both OpenAI LLMs as well as open-sourced LLMs like WizardCoder. We also conduct comprehensive ablation studies with different methods of prompting, number of clusters, model sizes, program qualities, etc., to provide useful insights that underpin CodeChain’s success.            Key Points By AI                                The long-term goal of artificial intelligence is to develop executable and functionally correct computer programs to solve complex problems.  Large pre-trained language models (LLMs) have made unprecedented progress in recent years and have expanded to learning large-scale code data.  Current state-of-the-art models cannot compare with skilled developers in highly complex programming tasks because their generation methods are too simple.  Most methods using LLMs adopt a simple generation approach, where the model usually generates code solutions as a single overall code block.  Another limitation is that the model generates a large number of solutions independently, hoping that one of them will pass all private test cases.  Recent research has proposed a method of subsampling output, providing the model with problem descriptions and public test cases as input.  Experienced developers will try to modify and reuse code developed previously until they are satisfied.  Some recent works have addressed this issue, for example, using LLM for self-correction, improving generated solutions through feedback such as compiler error messages, test results, and natural language explanations.  These methods only use independent feedback from each solution, ignoring the potential collective insights of all generated samples or their subcomponents.  Inspired by the problem-solving process, we propose CodeChain, a novel reasoning framework to improve code generation in LLMs through a chain of self-correcting submodules.  In CodeChain, we first introduce chain-of-thought prompts to indicate that the LLM should decompose solutions into modular paragraphs, each representing an abstract function for advanced logical subtasks.  To exploit modularity in the program, we further improve the generation process through a series of self-corrections, each based on a set of sampled submodules.  We first extract the submodules found in the generated program and group them into clusters, where we sample the central submodules and treat them as reusable code parts for self-correction.  Then, we expand the original chain-of-thought prompts through these selected submodules and instruct the LLM to generate new modular solutions.  In this way, the LLM can gain collective insights from the modular components of all past generated samples to improve its future generations, imitating the problem-solving process of experienced developers.  Experimental results show that CodeChain can significantly improve the performance of LLMs and achieve state-of-the-art performance on challenging code tasks in APPS and CodeContests.  CodeChain has improved the average pass@1 performance on APPS by over 35%, and on CodeContests by over 76%. We have also observed continuous improvements for both OpenAI LLM and open-source LLMs (such as WizardCoder).  We have also conducted comprehensive ablation studies, including analyzing single-step and multi-step revisions, feedback types, and cluster numbers, to gain useful insights into success of CodeChain.  Our work is closely related to the research of large-scale Transformer-based language models (LLMs).  Originally designed for natural language processing, these models have been expanded to learn large-scale code data and are skilled at understanding the context of programming languages and generating outputs.  More directly related is recent work aimed at improving code generation quality through output feedback, which introduced a simple filtering method by only selecting output samples that passed public test cases.  Some related work aims to improve generation quality through iterative self-revision, which uses the test results of public test cases as feedback for the model to revise its code.    File    [2310.08992] CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules (arxiv.org)  "
  },
  
  {
    "title": "【DAILY READING】Augmented Language Models：a Survey",
    "url": "/posts/Augmented-Language-Models-a-Survey/",
    "categories": "paper",
    "tags": "daily reading",
    "date": "2023-10-29 20:15:39 +0800",
    





    
    "snippet": "Conclusion By MyselfTable of ContentsAbstractThis survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools.The former is defined as decom...",
    "content": "Conclusion By MyselfTable of ContentsAbstractThis survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools.The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter.LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.1 Introduction: motivation for the survey and definitions1.1 Motivation  more reading                                                      [ ] Evaluating large language models trained on code              copilot                                                                                      [ ] Neural text generation with unlikelihood training. In International Conference on Learning Representations              hallucinations                                            fundamental defect of today’s LLMs          a single parametric model      a limited context        external things can integrate into LLMs          external documents      external data sources      external tools        definitions in this survey          Reasoning: reasoning is decomposing a potentially complex task into simpler subtasks the LM can solve more easily by itself or using tools.      Tool: a tool is an external module that is typically called using a rule or a special token and whose output is included in the ALM’s context.      Act: we will sometimes denote the call of a tool by an ALM as an action, even if it does not have an external effect.      Why jointly discussing reasoning and toolsWhy jointly discussing tools and actions1.2 Our classification2 Reasoning2.1 Eliciting reasoning with promptingelicitive prompts encourage LMs to solve tasks by following intermediate steps before predicting the output/answer.Few-shot setting                              [ ] Chain of thought prompting elicits reasoning in large language models.          Chain-of-Thought                                                  [ ] Measuring and narrowing the compositionality gap in language models          self-ask                      Webshop: Towards scalable real-world web interaction with grounded language agents. | ReAct    Zero-shot setting    few-shot provides examples of the task at hand, zero-shot conditions the LM on a single prompt that is not an example.    Large language models are zero-shot reasoners. | think step by step    2.2 Recursive prompting    2.3 Explicitly teaching language models to reason    all works also show that small scale instruction-finetuned models can perform better than un-finetuned large scale models, especially in the tasks where instruction following is important    2.4 Comparison and limitations of abstract reasoning    overall, reasoning can be seen as decomposing a problem into a sequence of sub-problems either iteratively or recursively.    Hypertree proof search for neural theorem proving. | example of other reasoning structures such as trees could be considered    3 Using Tools and Act    The possibility to easily include tools and actions in the form of special tokens is a convenient feature of language modeling coupled with transformers.    3.1 Calling another model    Iterative LM calling                                [ ] Re3: Generating longer stories with recursive reprompting and revision.          seems interesting                                                  [ ] Diffusion-lm improves controllable text generation.          a model can denoising sequence of Gaussian vectors into word vectors                      Peer: A collaborative language model. | a model can call itself repeatedly    Leveraging other modalities    Flamingo: a visual language model for few-shot learning.  Socratic models: Composing zero-shot multimodal reasoning with language | a modular framework allows models exchange information with each other and acquire new multimodal capabilities without additional finetuning    3.2 Information retrieval    3.2.1 Retrieval-augmented language models    Dense and sparse retrievers    both types of retrievers assess the relevance of a document to an information-seeking query. this can be done by:          checking for precise term overlap, or      computing the semantic similarity across related concepts.        Conditioning LMs on retrieved documents        Chain-of-thought prompting and retrievers        3.2.2 Querying search engines        In general, reasoning can improve decision making by making better inferences and predictions, while the ability to use external tools can improve reasoning by gathering additional information from knowledge bases or environments        3.2.3 Searching and navigating the web              Webgpt: Browser-assisted question-answering with human feedback. | a LM-based agent, can search the internet, navigate webpages, follow links, and cite sources    3.3 Computing via Symbolic Modules and Code Interpreters    3.4 Acting on the virtual and physical world    Controlling Virtual Agents    Controlling Physical Robots    4 Learning to reason, use tools, and act    4.1 Supervision    Few-shot prompting    Fine-tuning    Prompt pre-training    Bootstrapping    4.2 Reinforcement learning    Hard-coded reward functions    Human feedback    RLHFs:          Tamer: Training an agent manually via evaluative reinforcement.      Interactive learning from policy-dependent human feedback.      Deep reinforcement learning from human preferences.      Deep tamer: Interactive agent shaping in high-dimensional state spaces.        4.3 Limitations and future directions        5 Discussion        Moving away from language modeling        A tradeoff between memorizing and querying tools        Generalizing the non-parametric framework        A path towards autonomous machine intelligence              A path towards autonomous machine intelligence | LeCnn’s    Augmented Language Models benefits    Truthfulness    Estimating and reducing uncertainty    Interpretability    Enhanced capabilities    Ethical concerns    File    [2302.07842] Augmented Language Models: a Survey (arxiv.org)  "
  },
  
  {
    "title": "【DAILY READING】Textbooks Are All You Need",
    "url": "/posts/Textbooks-Are-All-You-Need/",
    "categories": "paper",
    "tags": "daily reading",
    "date": "2023-10-29 20:15:33 +0800",
    





    
    "snippet": "Summary of total paperThis paper gives an opinion is that the quality of the data has a higher cost performance on the size of model on specific targets now.It proved this point of view by using Co...",
    "content": "Summary of total paperThis paper gives an opinion is that the quality of the data has a higher cost performance on the size of model on specific targets now.It proved this point of view by using Code as an example. A 1.3B model, pre-trained and finetuned on some textbooks-data, gives an extraordinary performance on HumanEval and MBPP.AbstractWith the emerging of Transformer architecture, there came out a phenomenon that if we want to improve the performance of the model, just get more data and enlarge the size of the model.But we found that if the quality of the data is good enough, we will get a remarkable model in a smaller size with fewer data.By this way, we got two models. One is 1.3B, pretrained with 7B tokens of “textbook quality” data and finetuned on 200M tokens of “textbook-exercise-like” data. And another is more smaller. Both of them get a unbelievable performance on HumanEval and MBPP.Training details and the importance of high-quality dataSome drawbacks of common code data:  not self-contained, depend on other external files or modules.  Typical examples don’t have logically compute, but rather consist of meaningless code such as constants, parameters, or GUI elements.  Some good codes are often buried inside complex or poorly documented functions.  Some examples are skewed onto certain topics or use cases, will result an unbalanced distribution of dataset.Anyway, the common code data have so many noises.Datasets we used:  A filtered code-language dataset from The Stack and StackOverflow. Obtained by using a language model-based classifier. (About 6B tokens)  A Python textbook dataset generated by GPT-3.5. (About 1B tokens)  A small synthetic exercises dataset of Python exercises and solutions. (About 180M tokens)    Filtering of existing code datasets using a transformer-based classifier    Raw data is the Python subset of the deduplicated version of The Stack and StackOverflow, which contains over 35 million files/samples, counting over 35B tokens.Then we annotate the quality of a small subset of these files using GPT-4 by giving a code snippet to it with a prompt of determine its educational value for a student whose goal is to learn basic coding concepts.We then use this annotated dataset to train a random forest classifier to predict the quality of code.Note that we use GPT-3.5 to generate data but use GPT-4 to annotations the quality only.    Creation of synthetic textbook-quality datasets    It is so important to keep the diversity of the dataset, but it is also hard. Because language models tend to follow the most probable or common paths given their training data and their priors. So they are tend to give some more related outputs.Inspired by, we look for ways to inject randomness into the prompt in a way that gives rise to the generation of a diverse dataset.    The synthetic textbook dataset          Here, diversity is obtained by providing constraints on topics and target audience of the generated textbook.        The CodeExercises dataset          Here, the main means of eliciting diversity is by constraining the function names.      In particular, we conduct explicit decontamination and alternative evaluations to ensure that problems similar to those from HumanEval benchmark are not seen during finetuning.        Model architecture and training              A decoder only transformer model using the FlashAttention implementation of multi-head attention (MHA).  Use MHA and MLP layers in parallel configuration following some recent models like CodeGen, PaLM, and GPT-NeoX.  The 1.3B/350M model consists 24/20 layers, hidden dimension of 2048/1024, MLP-inner dimension of 8192/4096, 32/16 attention heads of dimension 64 each.  Use a rotary position embedding with rotary dimension 32.  Aside from FlashAttention, the models do not use other techniques like Fill-In-the-Middle (FIM), or Multi-Query-Attention (MQA) that could further boost performance and efficiency.  The models are trained on sequence length of 2048 with next-token prediction loss.  Use fp16 training with AdamW optimizer, linear-warmup-linear-decay learning rate schedule, and attention and residual dropout of 0.1.  Trained on 8 Nvidia-A100 with about 4 days. And finetuned about 7 hours.Pretraining:Trained on the CodeTextbook dataset with batch size 1024, maximum learning rate 1e-3, with warmup over 750 steps and weight decay 0.1, for a total of 36,000 steps. (Used the checkpoint as 24,000 steps)Finetuning:Finetuned with the same setup as pretraining, but different hyper-parameters: batch size of 256, maximum learning rate 1e-4 with 50 steps warmup, and weight decay 0.01 for total 6,000 steps.    Spikes of model capability after finetuning on CodeExercises    It suggests that the finetuning process might have helped the model in reorganizing and consolidating the knowledge acquired during pretraining. Even if such knowledge is not explicitly present in the CodeExercises dataset.    Finetuning improves the model’s understanding    Finetuning improves the model’s ability to use external libraries    Even include the libraries that the exercises do not contains.    Evaluation on unconventional problems with LLM grading    We evaluated the generating code not only by running it out, but also examines the reasoning and the quality of the solution by GPT-4.    Data pruning for unbiased performance evaluation    Try to avoid the pre-training data consists the data in HumanEval, we removed the duplicated data by N-gram overlap search, and computed the similarity on Embedding and syntax-based similarity analysis. We used L2 distance on Embedding, and abstract syntax trees (ASTs) on syntax.    File    [2306.11644] Textbooks Are All You Need (arxiv.org)  "
  },
  
  {
    "title": "【DAILY READING】Efficient Estimation of Word Representations in Vector Space",
    "url": "/posts/Efficient-Estimation-of-Word-Representations-in-Vector-Space/",
    "categories": "paper",
    "tags": "daily reading",
    "date": "2023-10-29 20:15:27 +0800",
    





    
    "snippet": "Conclusion By MyselfThis paper introduced two model architectures to deal with the problems when computing the word vectors. The two architectures are CBOW and skip-gram. Specifically, the CBOW is ...",
    "content": "Conclusion By MyselfThis paper introduced two model architectures to deal with the problems when computing the word vectors. The two architectures are CBOW and skip-gram. Specifically, the CBOW is predicts the word in the middle by using the words around it. Conversely, the skip-gram is uses the middle word to predict other words around.This paper is seen as the first word embedding model nowadays.AbstractWe propose two novel model architectures for computing continuous vector representations of words from very large data sets.The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks.We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set.Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.Key Points By AI  Many current NLP systems and techniques tread words as atomic units - there is no notion of similarity between worlds, as these are represented as indices in a vocabulary.  An examples is the popular N-gram model used for statistical language modeling - today it is possible to train N-grams on virtually all available data.  We focus on distributed representations of words learned by neural networks, as it was previously shown that they perform significantly better than Latent Semantic Analysis for preserving linear regularities among words; Latent Dirichlet Allocation becomes computationally very expensive on large data sets.  In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks.  The neural network based word vectors were previously applied to many other NLP tasks, for example sentiment analysis and paraphrase detection.  It can be expected that these applications can benefit from the model architectures described in this paper.    File    [1301.3781] Efficient Estimation of Word Representations in Vector Space (arxiv.org)  "
  },
  
  {
    "title": "【DAILY READING】Deep contextualized word realizations",
    "url": "/posts/Deep-contextualized-word-realizations/",
    "categories": "paper",
    "tags": "daily reading",
    "date": "2023-10-29 20:15:20 +0800",
    





    
    "snippet": "Conclusion By MyselfThe traditional technology of word representation is use an extra model to train a stable word vectors. ELMo can give a dynamic word representations by a language model. This LM...",
    "content": "Conclusion By MyselfThe traditional technology of word representation is use an extra model to train a stable word vectors. ELMo can give a dynamic word representations by a language model. This LM is given by the architecture of biLSTM.When we use this model, the word “apple” in “I’d like to eat apple.” and “I like the products of apple.” will give back in two different vectors.AbstractWe introduce a new type of deep contextualized word representation that models both:  complex characteristics of word use (e.g., syntax and semantics), and  how these uses vary across linguistic contexts (i.e., to model polysemy).Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus.We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.    Key Points          Pre-trained word representations are a key component in many neural language understanding models      We use vectors derived from a bidirectional Long Short-Term Memory that is trained with a coupled language model objective on a large text corpus      We show that the higher-level Long Short-Term Memory states capture context-dependent aspects of word meaning while lower-level states model aspects of syntax      We show that similar signals are induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision      Question answering The Stanford Question Answering Dataset (SQuAD) contains 100k+ crowd sourced question-answer pairs where the answer is a span in a given Wikipedia paragraph      We have introduced a general approach for learning high-quality deep context-dependent representations from bidirectional language model, and shown large improvements when applying ELMo to a broad range of NLP tasks        File        [1802.05365] Deep contextualized word representations (arxiv.org)            "
  },
  
  {
    "title": "【DAILY READING】iTransformer：Inverted Transformer Are Effective for Time Series Forecasting",
    "url": "/posts/iTransformer-Inverted-Transformer-Are-Effective-for-Time-Series-Forecasting/",
    "categories": "paper",
    "tags": "daily reading",
    "date": "2023-10-29 20:15:17 +0800",
    





    
    "snippet": "Conclusion By MyselfHonestly, this paper is not so clear to me. So I came back to this article Transformer王者归来！ where introduced the paper to me.This is a specific aspect of transformer on time ser...",
    "content": "Conclusion By MyselfHonestly, this paper is not so clear to me. So I came back to this article Transformer王者归来！ where introduced the paper to me.This is a specific aspect of transformer on time series, or we can say it is a model for future forecasting. This paper is purpose to deal with the problem of linear forecasting models are overtaking the transformer architecture models. It gives an opinion that the transformers now is underdeveloped, and it introduces a iTransformer model which is simply inverting the duties of the attentions mechanism and the feed-forward network.More detailed, it changes the representations on layer-normal, feed-forward, and attention. But it is not able to be understood for me now. OK, it is still too naive for me on AI. Let’s read more, study more.AbstractThe recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters.These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each formed by multiple variates of the same timestamp.However, Transformer is challenged in forecasting series with large lookback windows due to performance degradation and computation explosion.Besides, the unified embedding for each temporal token fuses multiple variates with potentially unaligned timestamps and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps.In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any adaptation on the basic components.We propose iTransformer that simply inverts the duties of the attention mechanism and the feed-forward network.Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward is applied for each variate token to learn nonlinear representations.The iTransformer model achieves consistent state-of-the-art on several real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting.File[2310.06625] iTransformer: Inverted Transformers Are Effective for Time Series Forecasting (arxiv.org)"
  },
  
  {
    "title": "【DAILY READING】DistilBERT, a distilled version of BERT：smaller, faster, cheaper and lighter",
    "url": "/posts/DistilBERT,-a-distilled-version-of-BERT-smaller,-faster,-cheaper-and-lighter/",
    "categories": "paper",
    "tags": "daily reading",
    "date": "2023-10-29 20:15:11 +0800",
    





    
    "snippet": "Conclusion By MyselfThis paper is about distilling BERT model. It is not so hard to read, but because the lacking of the distilling knowledge, I still can not clearly know what it said, Aha.It intr...",
    "content": "Conclusion By MyselfThis paper is about distilling BERT model. It is not so hard to read, but because the lacking of the distilling knowledge, I still can not clearly know what it said, Aha.It introduces “a triple loss”, “cosine-distance losses”, I don’t what it is, and the paper says nothing about it either. (May it said, but I missed it.)But this gives a light that distilling can indeed help me with the problems in my real work now. Let’s do more research.AbstractAs Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remain challenging.In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts.While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster.To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses.Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.Key Points By AI  This report introduces the recent rise of transfer learning in the field of natural language processing and the application of large-scale pre-trained language models in NLP tasks. These models typically have hundreds of billions of parameters, and current pre-trained model research indicates that training larger models still improves the performance of downstream tasks. However, this trend has also brought several concerns, including the environmental burden and the increasing computational and memory requirements.  To address these concerns, this paper introduces a method for training smaller language models using knowledge distillation. This method improves the performance of inference time while requiring less computational training budget. Our general pre-trained model can be used for multiple downstream tasks and retains the flexibility of a large model. We also demonstrate that our compressed model can be run on edge devices, such as mobile devices.  We use the triple loss method to train a 40% smaller knowledge distillation version of the Transformer, and ourselves train a larger Transformer language model. The method enables us to achieve similar performance on multiple downstream tasks while reducing inference time by 60%. Further ablation study shows that all triple loss components are key factors to achieve optimal performance. We also make the training code and weights public to allow other to experiment with the model.  Finally, we evaluate the performance of DistilBERT in general language understanding tasks and downstream tasks, and the results show that it performs well on some tasks. We further investigate the performance of DistilBERT in several downstream tasks, including a classification task and a question answering task. On the SQuAD, DistilBERT only trails the full BERT model by 3.5 percentage points.  This paper introduces DistilBERT, a general language model based on BERT pre-training that is 40% smaller and 60% faster than BERT. By analyzing the ablation study, we find that knowledge distillation can successfully train a general language model and DistilBERT has potential for edge applications.    File    [1910.01108] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter (arxiv.org)  "
  },
  
  {
    "title": "【DAILY READING】Distilling the Knowledge in a Neural Network",
    "url": "/posts/Distilling-the-Knowledge-in-a-Neural-Network/",
    "categories": "paper",
    "tags": "daily reading",
    "date": "2023-10-29 20:15:01 +0800",
    





    
    "snippet": "Conclusion By MyselfThis paper introduced that it is helpful to train a little model with a big model. The specific method is train a big model with a lot of data and make it performance good. Then...",
    "content": "Conclusion By MyselfThis paper introduced that it is helpful to train a little model with a big model. The specific method is train a big model with a lot of data and make it performance good. Then, we can train a little model with the same data but with an additional task that is beside the little model need to predict the real target of the features, and it also need to predict the “soft target” which is produced by the big model. More details, the soft target is the class probabilities produced by the big model.AbstractA very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then average their predictions.Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets.Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique.We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model.We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse.Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.Key Points By AI  Many insects have a larval form that is optimized for extracting energy and nutrients from the environment and a completely different adult form that is optimized for the very different requirements of traveling and reproduction  We have found that using the original training set works well, especially if we add a small term to the objective function that encourages the small model to predict the true targets as well as matching the soft targets provided by the cumbersome model  The Deep Neural Network produces a probability distribution over clusters of tri-phone states at each time and a decoder finds a path through the Hidden Markov Model states that is the best compromise between using high probability states and producing a transcription that is probable under the language model  The ensemble gives a small improvement on the ultimate objective of Word Error Rate due to the mismatch in the objective function, bug again, the improvement in WER achieved by the ensemble is transferred to the distilled model  For a deep acoustic model that is version of the one used by Android voice search, we have shown that nearly all of the improvement that is achieved by training an ensemble of deep neural nets can be distilled into a single neural net of the same size which is far easier to deploy  For really big neural networks, it can be infeasible even to train a full ensemble, but we have shown that the performance of a single really big net that has been trained for a very long time can be significantly improved by learning a large number of specialist nets, each of which learns to discriminate between the classes in highly confusable cluster    File    [1503.02531] Distilling the Knowledge in a Neural Network (arxiv.org)  "
  },
  
  {
    "title": "【DAILY READING】LLaMA：Open and Efficient Foundation Language Models",
    "url": "/posts/LLaMA-Open-and-Efficient-Foundation-Language-Models/",
    "categories": "paper",
    "tags": "daily reading",
    "date": "2023-10-29 20:14:55 +0800",
    





    
    "snippet": "Conclusion By MyselfThis is a really clearly paper.It introduces the LLaMA. LLaMA’s training approach is similar to the methods described in [[Brown et al., 2020]];[[Chowdhery et al., 2020]], and i...",
    "content": "Conclusion By MyselfThis is a really clearly paper.It introduces the LLaMA. LLaMA’s training approach is similar to the methods described in [[Brown et al., 2020]];[[Chowdhery et al., 2020]], and is inspired by the [[Chinchilla scaling laws]].Transformer is the base architecture of LLaMA. More specifically, it uses [[RMSNorma normalizing function]], replaces the ReLU non-linearity by the [[SwiGLU Activation function]], and use the [[Rotary Positional Embeddings]] instead of the absolute position embeddings.LLaMAs are trained using the [[AdamW optimizer]], ant it also tried the optimize of models, such as use the inspiration from [[Rabe and Staats]] and the backward from [[Dao et al.]] to reduce the memory usage and runtime.These papers above are readable for me.AbstractWe introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters.We train our models on trillions of tokens, and show that it is possible to train SOTA models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets.In particular, LLaMA-13B outperform GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B.We release all our models to the research community.Key Points By AI  LLMs trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a few examples  We include two book corpora in our training datasets: the Gutenberg Project, which contains books that are in the public domain, and the Books3 section of ThePile, a publicly available dataset for training large language models  While we have selected some of the standard benchmarks that are used by the language model community to indicate some of the issues with these models, these evaluations are not sufficient to fully understand the risks associated with these models  We presented a series of language models that are released openly, and competitive with SOTA foundation models  LLaMA-13B outperforms GPT-3 while being more than 10x smaller, and LLaMA-65B is competitive with Chinchilla-70B and PaLM-540B  We observe that our model is significantly better at performing co-reference resolution for the “their/them/someone” pronouns than for the “her/her/she” and “his/him/he” pronouns  We hope that releasing these models to the research community will accelerate the development of large language models, and help efforts to improve their robustness and mitigate known issues such as toxicity and bias    File    [2302.13971] LLaMA: Open and Efficient Foundation Language Models (arxiv.org)  "
  },
  
  {
    "title": "【DAILY READING】An Initial Exploration of Theoretical Support for Language Model Data Engineering. Part 1：Pretraining",
    "url": "/posts/An-Initial-Exploration-of-Theoretical-Support-for-Language-Model-Data-Engineering.-Part-1-Pretraining/",
    "categories": "paper",
    "tags": "daily reading",
    "date": "2023-10-29 20:14:48 +0800",
    





    
    "snippet": "Conclusion By MyselfThis is not a formal paper from conferences or journals, this is a blog from Yao Fu.This blog introduces a view that the data may have a more deeper influential to model than us...",
    "content": "Conclusion By MyselfThis is not a formal paper from conferences or journals, this is a blog from Yao Fu.This blog introduces a view that the data may have a more deeper influential to model than us know now.First, it introduce of grokking, grokking is the phenomenon of the model changes its performance from simple to complexity in a short time but after a long training time. I think it is a little like the concept of “emergent”? But not totally same. “Emergent” means the model get the ability to deal with the case it had never seen, the “Grokking” means the duration of the model changes better on common test data.Then, this blog elaborates how will the data influence the model in two points.  Data factors that influence speed of learning.  How to measure the speed of learning.The factors of data influence the speed of learning in three ways, format of data, curriculum of data and the mix method of the data when pre-train.Then it discusses the speed measuring of model learning from micro level to macro level.OK, this is about all I can understand of this blog. I listened others’ share of this blog, and I begin to know how poor my understanding is.Let’s read more, study more.    File    An Initial Exploration of Theoretical Support for Language Model Data Engineering. Part 1: Pretrainig (notion.site)  "
  }
  
]

